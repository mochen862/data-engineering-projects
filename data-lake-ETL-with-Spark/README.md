![data lake!](https://github.com/mochen862/data-engineering-projects/blob/main/data-lake-ETL-with-Spark/data%20lake.png)

# 1 Purpose of Sparkify's sparkifydb database and analytical goals

## 1.1 Purpose
- To build an ETL pipeline that extracts Sparkify's data from S3, process them using Spark, and load the data back into S3 as a set of dimensional tables

## 1.2 Analytical goals
- To allow Sparkify's analytics team to continue finding insights in what songs their users are listening to.

# 2 Database schema design and ETL pipeline

## 2.1 Database schema design

### 2.1.1 Project Datasets
- Sparkify's data resides in S3, in a directory of JSON logs on user activity on the app, and a directory with JSON metadata on the songs in their app
- Links for the two datasets that reside in S3
    - song data `s3://udacity-dend/song_data`
    - log data `s3://udacity-dend/log_data`
- Link for the log data json path `s3://udacity-dend/log_json_path.json`
- The **song dataset** is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.
    - file path example `song_data/A/A/B/TRAABJL12903CDCF1A.json`
    - what the above file looks like

`{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}`
- The **log dataset**  consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim)  based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings. The log files in the dataset are partitioned by year and month.
    - file path example `log_data/2018/11/2018-11-12-events.json`
    
![log dataset!](https://github.com/mochen862/data-engineering-projects/blob/main/data-lake-ETL-with-Spark/log%20dataset.png)

### 2.1.2 Schema for Song Play Analysis

#### Fact table
1. **songplays** - records in event data associated with song plays i.e. records with page `NextSong`
    - *songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent*
    
#### Dimension tables
2. **users** - users in the app
    - *user_id, first_name, last_name, gender, level*
3. **songs** - songs in the database
    - *song_id, title, artist_id, year, duration*
4. **artists** - artists in music database
    - *artist_id, name, location, lattitude, longitude*
5. **time** - timestamps of records in songplays broken down into specific units
    - *start_time, hour, day, week, month, year, weekday*
    
## 2.2 ETL Pipeline
1. Load song data and log data into dataframes using Spark
2. Create the songplays, songs, artists, time and users tables
3. Write them out to the selected s3 buckets (partitioned where necessary)
