![aws redshift diagram!](https://github.com/mochen862/data-engineering-projects/blob/main/aws-redshift-project/aws_redshift_diagram.png)

# 1 Purpose of Sparkify's sparkifydb database and analytical goals

## 1.1 Purpose
- to build an ETL pipeline that extracts Sparkify's data from S3, stage them in Redshift, and transform data into a set of dimensional tables to allow the analytics team to generate insights

## 1.2 Analytical goals
- to find what songs Sparkify's users are listening

# 2 Database schema design and ETL pipeline

## 2.1 Database schema design

### 2.1.1 Project Datasets
- Sparkify's data resides in S3, in a directory of JSON logs on user activity on the app, and a directory with JSON metadata on the songs in their app
- Links for the two datasets that reside in S3
    - song data `s3://udacity-dend/song_data`
    - log data `s3://udacity-dend/log_data`
- Link for the log data json path `s3://udacity-dend/log_json_path.json`
- The **song dataset** is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.
    - file path example `song_data/A/A/B/TRAABJL12903CDCF1A.json`
    - what the above file looks like
`{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}`
- The **log dataset**  consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim)  based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings. The log files in the dataset are partitioned by year and month.
    - file path example `log_data/2018/11/2018-11-12-events.json`

### 2.1.2 Schema for Song Play Analysis

#### Fact table
1. **songplays** - records in event data associated with song plays i.e. records with page `NextSong`
    - *songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent*
    
#### Dimension tables
2. **users** - users in the app
    - *user_id, first_name, last_name, gender, level*
3. **songs** - songs in the database
    - *song_id, title, artist_id, year, duration*
4. **artists** - artists in music database
    - *artist_id, name, location, lattitude, longitude*
5. **time** - timestamps of records in songplays broken down into specific units
    - *start_time, hour, day, week, month, year, weekday*
    
## 2.2 ETL Pipeline
1. Create staging, fact and dimension tables, as well as drop table statements, copy statements for the staging tables and insert statements for the fact and dimension tables in `sql_queries.py`
2. Use `create_tables.py` to connect to the database and create the tables
3. Launch a Redshift cluster and create an IAM role that has read access to S3
4. Run `create_tables.py` and check the tables schemas in Redshift database
5. Use `etl.py` to load data from S3 to Redshift staging tables
6. Use `etl.py` to load data from the staging tables to the fact and dimension tables on Redshift
7. Delete Redshift cluster when finished

# 3 Some example queries for song play analysis

Find Top 10 most played songs' song ids
`SELECT song_id, count(*)
FROM songplays
GROUP BY song_id
ORDER BY count(*) DESC
LIMIT 10;`

Find Top 10 most played artists' artist ids
`SELECT artist_id, count(*)
FROM songplays
GROUP BY artist_id
ORDER BY count(*) DESC
LIMIT 10;`

Free vs paid
`SELECT level, count(*)
FROM songplays
GROUP BY level;`

